In this chapter in \cref{exp:model_tests} we evaluate the proposed heuristic against the MILP model (\ref{sec:milp}) and in \cref{exp:literature_tests} against other heuristic from the literature. We then show the effectivness of our approach for our case study in \cref{exp:usecase_results}.
All the tests were run on a desktop computer with an AMD Ryzen-7 5800x processor with 8 cores at 3.8 GHz and 32GB of DDR4 system RAM with Windows 10. The algorithm was implemented in Java 11 and the model was run using the python APIs from CPLEX Optimization Studio 22.1.0.
In every test CPLEX was used with a maximum runtime of 1 hour.
Each evaluation against the heuristic lists both operational modes described in \cref{sec:placement_modes} listed as "Group By Hash" and "Single Placement".
All the instaces used in each section of this chapter are available at \url{https://github.com/artumino/BinPackingThesis/tree/main/tests/instances}.
Out of the 100 instances used for our case study experiments, only 80 were freely sharable with the generation procedure also described in \cref{exp:usecase_results}.

%TODO: Talk about time normalization?
\section{Model validation}
We compared our heuristic to the proposed MILP model of \cref{sec:milp} with a single bin and with no limit on the height of the bin (also referred to the 3D strip packing problem).
The heuristic was configured to run without vertex support, using only area support rules for its feasibility checks and $k$ was set to $200$.
The configured parameters for the test were $\alpha_s = 0.7$, $\beta_s = 5$, and the discretization unit for the model was $\delta = 10$. 
Tests were run on the first instance of the class 1 problems of the literature tests described in \cref{def:class1_instances}.
The test was run with an iterative approach by selecting only a limited ammout of items from the selected instance starting from 1 item and increasing the number of items to pack by one at each iteration.
Starting from instance with 6 boxes to pack, the solution from the previous instance of the problem was used to mip start the new one to allow for lower execution times.
Table \ref{exp:model} shows the obtained $z_{\text{max}}$ value of the heuristic and the MILP solution, the runtime in seconds and the number of items.
Since the underlying problem is NP-Hard it is shown that starting from instances of size bigger than 8 items, the MILP model becomes too slow for practical use while our heuristic mantains a negligible execution time.
Due to discretization errors, some of the model instances gave solutions that didn't have the expected amount of support and are marked with an asterix.
The solution to instance number 5 and instance number 7 is also shown in \cref{fig:model_tests}.
\label{exp:model_tests}
\input{tests/model}
\input{Images/tests/model/model}

\section{Literature results}
The heuristic was also evaluated against instances from the literature defined by \citeauthor{martello2000three}.
Since these instances were designed for heuristics without the vertical support constraint and without orthogonal rotations we ran the experiments with a relaxed version of our constraints.
The heuristic was configured to ignore the support constraint with $\alpha_s = 0$ and $\beta_s = 1$. We also disable orthogonal rotations and stopped scoring insertions based on the support area available (as described in \cref{ssec:scoring_insertions}).

\label{def:class1_instances}
The literature instances are divided in classes from 1 to 8 with each class having a different bin size and various distributions of types of items.
Instances were generated with the C++ instance generator provided by \citeauthor{martello2000three} which allows the generation of problem instances given a problem class and the number of items to use.
We generated 10 instances for each pair of problem class and number of items $n \in \{50, 100, 150, 200\}$.

In \cref{exp:literature_bins} we compare the average number of opened bins across 10 instaces for each problem class and $n$ number of items combination.
The results are then compared to the most effective methods from the literature listed as TS3 \citep{lodi2002heuristic}, GLS \citep{faroe2003guided}, GASP \citep{crainic2009ts2pack}, GVND \citep{parreno2010hybrid}, EHGH2 \citep{hifi2014hybrid}, BRKGA \citep{gonccalves2013biased}, BRKGA-VD \citep{zudio2018brkga} and ordered by publishing date.
Best values of all the heuristics are marked in bold. Best scoring values across different configurations of our heuristic are instead marked in italic.
Results show an average gap of $4.1\%$ compared to the average value across the other heuristics and an average gap of $5.32\%$ across the values from the best performing one.

In \cref{exp:literature_time_gap} we give an approximate comparison between the average execution time of our heuristic with respect to BRKGA-VD. Execution times for BRKGA-VD were normalized by comparing directly the floating point operations per seconds of the processors used, which resulted in dividing BRKGA-VD times by a normalization term of $9.3$. 
The times are averaged across all 8 classes of problem based on the size of the instance. As a last column we also included the average gap of each configuration of the heuristic with respect to the values of BRKGA-VD.
\label{exp:literature_tests}
\input{tests/literature_bins}
\input{tests/literature_time_gap}


\section{Case study results}
Case study experiments were conducted on a series of problem instances that were divided between 20 real world instances and 80 generated instances based on a sampling of an anagraphic of real world products.
Each instance was anonymized and converted to a format similar to the one used for the literature tests thanks to a Rust program available at \url{https://github.com/artumino/BinPackingThesis/tree/main/additional/testConverter}.
Support parameters for the heuristic was set to $\alpha_s = 0.7$ and $\beta_s = 10$ with both area and vertex support enabled, with dimensions for the bin, for the items and for the tolerances assumed to be in millimeters.
Different values of $k \in \{1, 5, 10, 20, 50, 100, 200\}$ were tested as well as both placement modes.

Each generated instance is composed of a random number of $N$ items sampled from a given range of possible instance sizes. All generated instances had a bin of standard size $800 \times 1200 \times 2000$.
We identified four ranges of interest and generated 20 instances for each range as follows:
\begin{itemize}
    \item \textbf{Class 1-20}: a class of instances with the target sizes for our case-study $N \in [70,100]$
    \item \textbf{Class 21-40}: a class of small sized instances with number of items $N \in [50,70]$
    \item \textbf{Class 41-60}: a class of medium sized instances with number of items $N \in [70,120]$
    \item \textbf{Class 61-80}: a class of big instances with number of items $N \in [120,200]$
\end{itemize}

Given an input $N$ (the size of the test instance), the generation procedure uniformly sampled an item type from an anagraphic of real-world products. The number of items of that type to add to the test instance was then sampled from a normal distribution $\mathcal{N}(\mu = 4.6, \sigma = 1.8)$, floored to be an integer value and clamped to avoid generating more items than $N$. 
This uniform sampling of item types was done until the instance was composed of $N$ items.

Real world instances are listed as \textbf{Class 81-100} and have a variable number of items between $[25, 345]$, a variable bin size (although similar to the one used for the generated instances), and a variable number of items of the same type with homogeneous instances with only a few unique types of items and instances with every item of a different type.
An example of real world instances is shown in \cref{fig:usecase_tests} where items of the same shape are marked with the same color.

Table \ref{exp:usecase_results} shows the average results over the 20 instances per class, divided by each configuration of the heuristic with different values of $k$. The results shown include the total execution time in milliseconds (TT), the number of opened bins (B) and the average cage ratio between the opened bins (CR). It is clear that although the "Single Placement" method had better results when dealing with a relaxed version of the problem, grouping items by type shows consideral improvements under all measured metrics when taking support into account.
Most of the configurations lead to an average cage ratio of more than $70\%$, which was the target value for our case study.
It is also possible to see that increasing the value of $k$ improves the quality of the solutions, on average, at the expense of a higher execution time.
By doing a case by case analysis of the each experiment we discovered that in some instances increasing $k$ can temporarily worsen the solution. 
A further study of the problematic instances highlighted that the current greedy scoring mechanism of the states can lead to cutting out good solutions too early. 
Further improvements are considered in \cref{chapther:conclusions}.
\label{exp:usecase_results}
\input{tests/use_case_avg.tex}
\input{Images/tests/usecase/usecase}