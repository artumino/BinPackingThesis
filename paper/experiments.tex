In this chapter, in \cref{exp:model_tests}, we evaluate the proposed heuristic against the MILP model (\ref{sec:milp}), and in \cref{exp:literature_tests} against other heuristics from the literature. We then show the effectiveness of our approach for our case study in \cref{exp:usecase_results}.
All the tests were run on a desktop computer with an AMD Ryzen-7 5800x processor with 8 cores at 3.8 GHz and 32GB of DDR4 system RAM with Windows 10. The algorithm was implemented in Java 11, and the model was run using the python APIs from CPLEX Optimization Studio 22.1.0.
In every test, CPLEX was used with a maximum runtime of 1 hour.
Each evaluation against the heuristic lists both operational modes described in \cref{def:placement_modes} listed as "PM" and "PS".
All the instances used in each section of this chapter are available at \url{https://github.com/artumino/BinPackingThesis/tree/main/tests/instances}.
Out of the 100 instances used for our case study experiments, only 80 were freely sharable with the generation procedure also described in \cref{exp:usecase_results}.

\section{Model validation}
We compared our heuristic to the proposed MILP model of \cref{sec:milp} with a single bin and with no limit on the height of the bin (also referred to as the 3D strip packing problem).
The heuristic was configured to run without vertex support, using only area support rules for its feasibility checks, and $k$ was set to $200$.
The configured parameters for the test were $\alpha_s = 0.7$, $\beta_s = 5$, and the discretization unit for the model was $\delta = 10$. 
Tests were run on the first instance of the class 1 problems from the literature tests described in \cref{def:class1_instances} which has a bin base of $100 \times 100$.
The test was run with an iterative approach by selecting only a limited amount of items from the selected instance, starting from 1 item and increasing the number of items to pack by one at each iteration.
The problem created with each iteration was saved as a test instance in the same format as the one used for literature tests. A python script then loaded each generated instance sequentially and evaluated the solutions from both the MILP problem and the heuristic.
Starting from instance 6, the solution from the previous instance was used to mip start the new one leading to lower execution times.
Table \ref{exp:model} shows the obtained $z_{\text{max}}$ value of the heuristic and the MILP solution, the runtime in seconds, and the number of items.
Since the underlying problem is NP-Hard, it is shown that starting from instances of size bigger than 8 items; the MILP model becomes too slow for practical use while our heuristic maintains negligible execution time.
Due to discretization errors, some of the model instances gave solutions that didn't have the expected amount of support and were marked with an asterisk.
The solution to instance number 5 and instance number 7 is also shown in \cref{fig:model_tests}.
\label{exp:model_tests}
\input{tests/model}
\input{Images/tests/model/model}

\section{Literature results}
The heuristic was also evaluated against instances from the literature defined by \citeauthor{martello2000three}.
Since these instances were designed for heuristics without the vertical support constraint and orthogonal rotations, we ran the experiments with a relaxed version of our constraints.
The heuristic was configured to ignore the support constraint with $\alpha_s = 0$ and $\beta_s = 1$. We also disabled orthogonal rotations and stopped scoring insertions based on the support area available (as described in \cref{ssec:scoring_insertions}).

\label{def:class1_instances}
The literature instances are divided into classes from 1 to 8, with each class having a different bin size and various distributions of types of items.
Instances were generated with the C++ instance generator provided by \citeauthor{martello2000three} which allows the generation of problem instances given a problem class and the number of items to use. %FIXME: with a given problem class and number of items to use
We generated 10 instances for each pair of problem class and number of items $n \in \{50, 100, 150, 200\}$ for a total of $320$ instances.

In \cref{exp:literature_bins} we compare the average number of opened bins across 10 instances for each problem class and $n$ number of items combination.
The results are then compared to the most effective methods from the literature ordered by publishing date and listed as TS3 \citep{lodi2002heuristic}, GLS \citep{faroe2003guided}, GASP \citep{crainic2009ts2pack}, GVND \citep{parreno2010hybrid}, EHGH2 \citep{hifi2014hybrid}, BRKGA \citep{gonccalves2013biased}, BRKGA-VD \citep{zudio2018brkga}.
The best values of all the heuristics are marked in bold. Best scoring values across different configurations of our heuristic are marked in italic instead.
Results show an average gap of $4.1\%$ compared to the average value across the other heuristics and an average gap of $5.32\%$ with respect to the best performing one.

In \cref{exp:literature_time_gap} we give an approximate comparison between the average execution time of our heuristic with respect to BRKGA-VD. Execution times for BRKGA-VD were normalized by comparing directly the floating-point operations per second of the processors used, which resulted in dividing BRKGA-VD execution times by a normalization term of $9.3$. 
The times are averaged across all 8 classes of problems based on the size of the instance. In the last column, we also included the average gap of each configuration of the heuristic with respect to the values of BRKGA-VD. %TODO: The values presented are the times averaged across 8 classes of problems divided according to the size of the instance and the heuristic configuration.
\label{exp:literature_tests}
\input{tests/literature_bins}
\input{tests/literature_time_gap}


\section{Case study results}
Case study experiments were conducted on a series of problem instances that were divided between 20 real-world instances and 80 generated instances composed of items sampled from a population of real-world products.
Each instance was anonymized and converted to a format similar to the one used for the literature tests thanks to a Rust program available at \url{https://github.com/artumino/BinPackingThesis/tree/main/additional/testConverter}.
Support parameters for the heuristic were set to $\alpha_s = 0.7$ and $\beta_s = 10$ with both area and vertex support enabled. All dimensions of the bin, items, and tolerances are assumed to be in millimeters.
Different values of $k \in \{1, 5, 10, 20, 50, 100, 200\}$ were tested as well as both placement modes.

Each generated instance is composed of a random number of $N$ items sampled from a given range of possible instance sizes. All generated instances had a bin of standard size $800 \times 1200 \times 2000$.
We identified four ranges of interest and generated 20 instances for each range as follows:
\begin{itemize}
    \item \textbf{Class 1-20}: a class of instances with the target sizes for our case-study $N \in [70,100]$
    \item \textbf{Class 21-40}: a class of small sized instances with number of items $N \in [50,70]$
    \item \textbf{Class 41-60}: a class of medium sized instances with number of items $N \in [70,120]$
    \item \textbf{Class 61-80}: a class of big instances with number of items $N \in [120,200]$
\end{itemize}

Given an input $N$ (the size of the test instance), the generation procedure uniformly sampled an item type from a population of real-world products. The number of items of that type to add to the test instance was then sampled from a normal distribution $\mathcal{N}(\mu = 4.6, \sigma = 1.8)$, floored to be an integer value and clamped to avoid generating more items than $N$. 
This uniform sampling of item types was done until the instance was composed of $N$ items.

Real-world instances are listed as \textbf{Class 81-100} and have a variable number of items between $[25, 345]$, a variable bin size (although similar to the one used for the generated instances), and a variable number of items of the same type. Some instances were homogeneous with only a few unique items, and some were heterogeneous with every item of a different type.
An example of real-world instances is shown in \cref{fig:usecase_tests} where items of the same shape are marked with the same color.

Table \ref{exp:usecase_results} shows the average results over the 20 instances per class, divided by each configuration of the heuristic with different values of $k$.
The results shown include the total execution time in milliseconds (TT), the number of opened bins (B), and the average cage ratio between the opened bins (CR).
It is clear that although the "PS" method had better results when dealing with a relaxed version of the problem, grouping items by type shows considerable improvements under all measured metrics when taking vertical support into account.
Most of the configurations lead to an average cage ratio of more than $70\%$, which was the target value for our case study.
It is also possible to see that increasing the value of $k$ improves the quality of the solutions, on average, at the expense of a higher execution time.
By doing a case-by-case analysis of each experiment, we discovered that increasing $k$ can temporarily worsen the solution in some instances. 
A further study of the problematic instances highlighted that the current greedy scoring mechanism of the states leads to cutting out good solutions too early. 
Further improvements are considered in \cref{chapther:conclusions}.
\label{exp:usecase_results}
\input{tests/use_case_avg.tex}
\input{Images/tests/usecase/usecase}